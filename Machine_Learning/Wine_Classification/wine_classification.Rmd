---
title: "wine_classification"
author: "Kaiwen Liu"
date: "11/1/2021"
output: html_document
---

## Load Data
```{r}
# load data
red <- read.csv("./winequality-red.csv",header = T,sep = ";")
white <- read.csv("./winequality-white.csv",header = T,sep = ";")

# 1 : red wine , 0 : white wine 
category <- c(rep(1,dim(red)[1]),rep(0,dim(white)[1]))
wine <- rbind(red,white)
```

## Preliminary Checks

### Normality

#### 1. normality plot

```{r}
# for red wine
# plot_hist <- function(data){
#   hist(scale(data))
# }
# plot_hist(red$fixed.acidity)
# plot_hist(red$quality)
# plot_hist(red$volatile.acidity)
# plot_hist(red$chlorides)
# plot_hist(red$residual.sugar)
# plot_hist(red$total.sulfur.dioxide)
```

#### 2.Shapiro Wilk test for normality
```{r}
sha_tests <- c()
for(i in 1:dim(red)[2]){
  sha_tests <- append(sha_tests,shapiro.test(red[,i])$p.value)
}

# an approximate p-value for the test. This is said in Royston (1995) to be adequate for p.value < 0.1.
which(sha_tests >= 0.1)
```




### Collinearity 

#### Collinearity of raw data
```{r}
# check collinearity 
cor(wine)
```

```{r}
# centering
wine_cen <- scale(wine,center=T,scale = F)
# cor after centering
cor(wine_cen)
```

### variable selsction

```{r}
library(leaps)

# select optimal model by certain method
forward_select <- function(X,Y,method){
  # models : models got from regsubsets
  # method : methods for choosing best p. ("AIC","BIC")
  
  # get models using forward by regsubsets
  models <- regsubsets(x = X,y = Y,method = "forward")

  # train y_hat for each model
  y_hat_trained <- function(coefs){
  # select columns with certain predictor
  col_i <- colnames(X)[which(coefs[-1])]
  X_train_i <- X[col_i]
  # train model with certain predictor
  fit_lm_i <- lm(Y ~ .,data = X_train_i)
  return(predict(fit_lm_i,X_train_i))
  }
  
  # get logical map of each model
  p_mat <- summary(models)$which
  # RSS of each model
  rss_i <- summary(models)$rss
  

  # iterate each model and get train_err and bic
  trainErrs <- c()
  # evaluation value  
  eva_vals <- c()
  # number of variables
  ps <- c()
  # number of sample size
  n <- dim(X)[1]
  # number of models
  num_models <- dim(summary(models)$which)[1]

  # func for different methods
  evaluate_mehtod_func <- function(){}
  if(method == "AIC"){
    evaluate_mehtod_func <- function(n,trainErr_i,pi){
      return(n*log(trainErr_i) + 2*pi)
    }
  }else if(method == "BIC"){
    evaluate_mehtod_func <- function(n,trainErr_i,pi){
      return(n*log(trainErr_i) + pi*log(n))
    }
  }else{
    message("Method input error!")
    return(NULL)
  }
  # iterate to get each models evaluation error
for (i in 1:num_models){
  pi <- sum(p_mat[i,]) 
  y_i <- y_hat_trained(p_mat[i,])
  trainErr_i <- mean((y_i - Y)^2)
  trainErrs <- append(trainErrs,trainErr_i)
  eva <- evaluate_mehtod_func(n,trainErr_i,pi)
  eva_vals <- append(eva_vals, eva)
  ps <- append(ps,pi-1)
}
  
  results <- list(num_variables = ps, models = models,MSEs = trainErrs,method_res = eva_vals)
  return(results)
  
}


# bic_table <- data.frame(t(rbind(ps,trainErrs,bics)));bic_table
# plot(bic_table$ps,bic_table$bics,xlab = "number of variables",ylab = "BIC")
```


```{r}
forward_results <- forward_select(wine,category,"AIC")

# model after forward selection 
opt_model <- which.min(forward_results$method_res)
coef <- summary(forward_results$models)$which[opt_model,][-1]
opt_vars <- names(subset(coef,coef==T))
```



```{r}
which.min(forward_results$method)
summary(forward_results$models)
```


## LDA

### LDA function using CV
```{r}
library(MASS)
# lda cv func
lda_cv <- function(x,cl,k) {
  # x : training data
  # cl : labels
  # k : number of folds
  n <- dim(x)[1]
  # runs 
  runs <- n %/% k
  rem <- n %% k
  
  preds <- c()
  # calculate each part
  for (i in 1:k) {
  # test sub index
  s.low <- (i-1)*runs + 1
  s.high <- i*runs
  # plus remainder in last run
  if(i==k){
    s.high <- i*runs + rem
  }
  
  # test set
  test.i <- x[s.low:s.high,]
  # train set
  train.i <- x[ -s.low:-s.high, ]
  # train label
  train.lab.i <- cl[-s.low:-s.high]

  # apply lda 
  zi <- lda(train.i,grouping = train.lab.i)
  pred.i <- predict(zi,test.i)$class
  preds <- append(preds,pred.i)
  }
  return(mean(preds != cl))
}

```


### apply LDA with raw data
```{r}
# cv error
# full model
lda_cv(wine,category,10)
```

### apply LDA with centering data and forwarded selected variables
```{r}
# model after forward selection
wine_opt <- wine[,match(opt_vars,colnames(wine))]
lda_cv(wine_opt,category,10)
```

## KNN

```{r}
# divide it into training data and test data
wine_lab <- cbind(wine,category)
set.seed(123)
n <- dim(wine_lab)[1]
test_ind <- sample(1:n,floor(n/5))
# test and train data with labels
test_wine_lab <- wine_lab[test_ind,]
train_wine_lab <- wine_lab[-test_ind,]

# test and train data with labels
test_wine <- test_wine_lab[,1:12]
train_wine <- train_wine_lab[,1:12]
# labels for test and train
test_labs <- test_wine_lab$category
train_labs <- train_wine_lab$category
```

```{r}
library(class)
# set knn error result function
gen_knn_err <- function(k.list,train_data,test_data,train_label,test_label){
  # return test error list for different k
  test_err = list()
  for (ki in k.list) {
    knn_i_train <- knn(train_data,test_data,train_label,k=ki)
    knn_i_error <- mean(knn_i_train != test_label)
    test_err <- append(test_err,knn_i_error)
  }
  return(test_err)
}
```

```{r}
k_list <- 1:15
knn_err <- gen_knn_err(k_list,train_wine,test_wine,train_labs,test_labs)
plot(k_list,knn_err)
```
```{r}
which.min(knn_err)
```


## Logistic

```{r}
# logistic regression cv func 
logistic_cv <- function(x,cl,k) {
  
  # x : training data
  # cl : labels
  # k : number of folds
  x <- data.frame(cbind(cl,x))
  x$cl <- as.factor(x$cl)
  
  n <- dim(x)[1]
  # runs 
  runs <- n %/% k
  rem <- n %% k

  preds <- c()
  
  for (i in 1:k){
      # test sub index
      s.low <- (i-1)*runs + 1
      s.high <- i*runs
      # plus remainder in last run
      if(i==k){
        s.high <- i*runs + rem
      }
      
      # test set
      test.i <- x[s.low:s.high,]
      # train set
      train.i <- x[ -s.low:-s.high, ]
      
      # apply logistic
      logist.i <- glm(formula(train.i), family = binomial(link = "logit"),maxit = 100, data = train.i)
      pred.i <- predict(logist.i,test.i)
      preds <- append(preds,pred.i)
  }
  return(mean((preds>0)*1 != cl))
}
```

```{r}
logistic_cv(wine,category,5)

```


